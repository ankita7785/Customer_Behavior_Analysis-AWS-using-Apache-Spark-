{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUH10nk_3CWZ"
   },
   "source": [
    "# Customer Behaviour Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzsjLp43zEFw"
   },
   "source": [
    "# Objective\n",
    "In this case study, you will be working on E-commerce Customer Behavior Analysis using Apache Spark, a powerful distributed computing framework designed for big data processing. This assignment aims to give you hands-on experience in analyzing large-scale e-commerce datasets using PySpark. You will apply techniques learned in data analytics to clean, transform, and explore customer behavior data, drawing meaningful insights to support business decision-making. Apart from understanding how big data tools can optimize performance on a single machine and across clusters, you will develop a structured approach to analyzing customer segmentation, purchase patterns, and behavioral trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsiRheDOzFh1"
   },
   "source": [
    "# Business Value\n",
    "E-commerce businesses operate in a highly competitive market where understanding customer behavior is critical to driving growth and retention. To stay ahead, companies must leverage data-driven insights to optimize marketing strategies, personalize customer experiences, and improve product offerings. In this assignment, you will analyze e-commerce transaction data to uncover patterns in purchasing behavior, customer preferences, and sales performance. With Apache Spark's ability to handle large datasets efficiently, businesses can process vast amounts of customer interactions in real-time, helping them make faster and more informed decisions.\n",
    "As an analyst at an e-commerce company, your task is to examine historical transaction records and customer survey data to derive actionable insights that can drive business growth. Your analysis will help identify high-value customers, segment users based on behavior, and uncover trends in product demand and customer engagement. By leveraging big data analytics, businesses can enhance customer satisfaction, improve retention rates, and maximize revenue opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTIS-7h9zG0b"
   },
   "source": [
    "# Assignment Tasks\n",
    "1. Data Preparation\n",
    "2. Data Cleaning\n",
    "3. Exploratory Data Analysis\n",
    "4. Customer Segmentation (RFM Analysis) and Business Insights\n",
    "5. Evaluation and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DG9z6-swb2Q"
   },
   "source": [
    "\n",
    "# Dataset Overview\n",
    "The dataset can be accessed the following [link](https://drive.google.com/drive/folders/1mBgC5tvZrh1bIBvpXVP_j-au5LFUAwOZ?usp=sharing).\n",
    "\n",
    "The dataset used in this analysis comprises longitudinal purchase records from 5,027 Amazon.com users in the United States, spanning 2018 to 2022.\n",
    "\n",
    "It is structured into three CSV files (amazon-purchases.csv, survey.csv, and fields.csv) that capture transactional data, demographic profiles, and survey responses.\n",
    "\n",
    "Collected with informed consent, the dataset enables analysis of customer behavior, product preferences, and demographic trends.\n",
    "\n",
    "**NOTE**: Personal identifiers (PII) were removed to ensure privacy, and all data were preprocessed by users before submission.\n",
    "\n",
    "`Data Dictionary:`\n",
    "\n",
    "| **Attribute**          | **Description** |\n",
    "|------------------------|----------------|\n",
    "| **Order Dates**        | The specific dates when orders were placed, enabling chronological analysis of sales trends. |\n",
    "| **Title** |The name of the product purchased. |\n",
    "|**Category** | The classification or group to which the product belongs, facilitating category-wise analysis. |\n",
    "| **Pricing** | The cost per unit of each product, essential for revenue calculations and pricing strategy assessments. |\n",
    "| **Quantities** | The number of units of each product ordered in a transaction, aiding in inventory and demand analysis. |\n",
    "| **Shipping States**    | The states to which products were shipped, useful for geographical sales distribution analysis. |\n",
    "| **Survey ResponseID**  | A unique identifier linking purchases to customer survey responses, enabling correlation between purchasing behavior and customer feedback. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fokhVB-gOv1N"
   },
   "source": [
    "# Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.4\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets==3.3.2\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib==3.8.4\n",
      "  Downloading matplotlib-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn==0.13.2\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm==4.67.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /mnt/notebook-env/lib/python3.9/site-packages (from datasets==3.3.2) (3.8.6)\n",
      "Requirement already satisfied: filelock in /mnt/notebook-env/lib/python3.9/site-packages (from datasets==3.3.2) (3.12.4)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /mnt/notebook-env/lib/python3.9/site-packages (from datasets==3.3.2) (23.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /mnt/notebook-env/lib/python3.9/site-packages (from datasets==3.3.2) (6.0.1)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /mnt/notebook-env/lib/python3.9/site-packages (from pandas==1.5.3) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /mnt/notebook-env/lib/python3.9/site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /mnt/notebook-env/lib/python3.9/site-packages (from matplotlib==3.8.4) (6.1.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.59.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8 in /mnt/notebook-env/lib/python3.9/site-packages (from matplotlib==3.8.4) (10.0.1)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/notebook-env/lib/python3.9/site-packages (from aiohttp->datasets==3.3.2) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/notebook-env/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (4.8.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /mnt/notebook-env/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.8.4) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/notebook-env/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/notebook-env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==3.3.2) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/notebook-env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==3.3.2) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/notebook-env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==3.3.2) (3.4)\n",
      "Building wheels for collected packages: pyspark, pyarrow\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849792 sha256=2091193910b10715c4c48b37ff2a03c01b105088f486f4af5deb1d9a815a9e70\n",
      "  Stored in directory: /home/emr-notebook/.cache/pip/wheels/b9/0f/0a/1bf9096f5b49f278182d7fe905a82209f2090edb24a7352b72\n",
      "  Building wheel for pyarrow (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyarrow \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[875 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   corresp(dist, value, root_dir)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/config/_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Pattern '../LICENSE.txt' cannot contain '..'\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please ensure the files specified are contained by the root\n",
      "  \u001b[31m   \u001b[0m         of the Python package (normally marked by `pyproject.toml`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/specifications/glob-patterns/ for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Cannot find any files for the given pattern.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Pattern '../LICENSE.txt' did not match any files.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Pattern '../NOTICE.txt' cannot contain '..'\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please ensure the files specified are contained by the root\n",
      "  \u001b[31m   \u001b[0m         of the Python package (normally marked by `pyproject.toml`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/specifications/glob-patterns/ for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/dist.py:483: SetuptoolsDeprecationWarning: Cannot find any files for the given pattern.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Pattern '../NOTICE.txt' did not match any files.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         By 2026-Mar-20, you need to update your project and remove deprecated calls\n",
      "  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   for path in sorted(cls._find_pattern(pattern, enforce_match))\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_compute_docstrings.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_generated_version.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/acero.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/benchmark.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/cffi.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/compute.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/conftest.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/csv.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/cuda.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/dataset.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/feather.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/flight.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/fs.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/ipc.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/json.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/jvm.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/orc.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/pandas_compat.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/substrait.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/types.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/util.py -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing pyarrow.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to pyarrow.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to pyarrow.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to pyarrow.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '../LICENSE.txt'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '../NOTICE.txt'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*.so' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '#*' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '.git*' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m no previously-included directories found matching '.asv'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'pyarrow.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.includes' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.includes' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.includes' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.includes' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.includes' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.interchange' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.interchange' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.interchange' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.interchange' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.interchange' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.parquet' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.parquet' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.parquet' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.parquet' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.parquet' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.src.arrow.python' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.src.arrow.python' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.src.arrow.python' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.src.arrow.python' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.src.arrow.python' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.src.arrow.python.vendored' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.src.arrow.python.vendored' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.src.arrow.python.vendored' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.src.arrow.python.vendored' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.src.arrow.python.vendored' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.feather' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.feather' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.feather' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.feather' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.feather' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.orc' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.orc' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.orc' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.orc' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.orc' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.data.parquet' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.data.parquet' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.data.parquet' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.data.parquet' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.data.parquet' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.interchange' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.interchange' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.interchange' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.interchange' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.interchange' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.tests.parquet' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.tests.parquet' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.tests.parquet' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.tests.parquet' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.tests.parquet' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m /mnt/tmp/pip-build-env-hzq_q0a2/overlay/lib/python3.9/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'pyarrow.vendored' is absent from the `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m         ############################\n",
      "  \u001b[31m   \u001b[0m         Python recognizes 'pyarrow.vendored' as an importable package[^1],\n",
      "  \u001b[31m   \u001b[0m         but it is absent from setuptools' `packages` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "  \u001b[31m   \u001b[0m         package, please make sure that 'pyarrow.vendored' is explicitly added\n",
      "  \u001b[31m   \u001b[0m         to the `packages` configuration field.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         Alternatively, you can also rely on setuptools' discovery methods\n",
      "  \u001b[31m   \u001b[0m         (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m         instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         If you don't want 'pyarrow.vendored' to be distributed and are\n",
      "  \u001b[31m   \u001b[0m         already explicitly excluding 'pyarrow.vendored' via\n",
      "  \u001b[31m   \u001b[0m         `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "  \u001b[31m   \u001b[0m         you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "  \u001b[31m   \u001b[0m         combination with a more fine grained `package-data` configuration.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "  \u001b[31m   \u001b[0m               even if it does not contain any `.py` files.\n",
      "  \u001b[31m   \u001b[0m               On the other hand, currently there is no concept of package data\n",
      "  \u001b[31m   \u001b[0m               directory, all directories are treated like packages.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/__init__.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_acero.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_acero.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_azurefs.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_compute.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_compute.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_csv.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_csv.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_cuda.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_cuda.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_orc.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dataset_parquet_encryption.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_dlpack.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_feather.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_flight.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_fs.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_fs.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_gcsfs.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_hdfs.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_json.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_json.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_orc.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_orc.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_parquet.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_parquet.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_parquet_encryption.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_parquet_encryption.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_pyarrow_cpp_tests.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_pyarrow_cpp_tests.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_s3fs.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/_substrait.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/array.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/benchmark.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/builder.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/compat.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/config.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/device.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/error.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/gandiva.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/io.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/ipc.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/lib.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/lib.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/memory.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/pandas-shim.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/public-api.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/scalar.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/table.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tensor.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/types.pxi -> build/lib.linux-x86_64-cpython-39/pyarrow\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/__init__.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/common.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_acero.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_dataset_parquet.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_feather.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_flight.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_fs.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_python.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libarrow_substrait.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libgandiva.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libparquet.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/includes/libparquet_encryption.pxd -> build/lib.linux-x86_64-cpython-39/pyarrow/includes\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/interchange/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/interchange/buffer.py -> build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/interchange/column.py -> build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/interchange/dataframe.py -> build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/interchange/from_dataframe.py -> build/lib.linux-x86_64-cpython-39/pyarrow/interchange\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/parquet/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/parquet/core.py -> build/lib.linux-x86_64-cpython-39/pyarrow/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/parquet/encryption.py -> build/lib.linux-x86_64-cpython-39/pyarrow/parquet\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/CMakeLists.txt -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/api.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_pandas.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_pandas.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/arrow_to_python_internal.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/async.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/benchmark.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/benchmark.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/common.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/common.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/csv.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/csv.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/datetime.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/datetime.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/decimal.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/decimal.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/extension_type.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/extension_type.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/filesystem.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/filesystem.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/flight.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/flight.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/gdb.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/gdb.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/helpers.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/helpers.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/inference.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/inference.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/io.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/io.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/ipc.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/ipc.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/iterators.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_convert.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_convert.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_init.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_init.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_internal.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_interop.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_to_arrow.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/numpy_to_arrow.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/parquet_encryption.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/parquet_encryption.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/platform.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow_api.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/pyarrow_lib.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_test.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_test.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_to_arrow.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/python_to_arrow.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/type_traits.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/udf.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/udf.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/util.cc -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/util.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/visibility.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python/vendored\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/vendored/CMakeLists.txt -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python/vendored\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/src/arrow/python/vendored/pythoncapi_compat.h -> build/lib.linux-x86_64-cpython-39/pyarrow/src/arrow/python/vendored\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_16597.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_39313.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/arrow_7980.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/bound_function_visit_strings.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/conftest.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/extensions.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/pandas_examples.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/pandas_threaded_import.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/read_record_batch.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/strategies.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_acero.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_array.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_builder.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cffi.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_compute.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_convert_builtin.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cpp_internals.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_csv.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cuda.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_cython.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dataset.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dataset_encryption.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_deprecations.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_device.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_dlpack.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_exec_plan.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_extension_type.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_feather.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_flight.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_flight_async.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_fs.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_gandiva.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_gdb.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_io.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_ipc.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_json.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_jvm.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_memory.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_misc.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_orc.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_pandas.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_scalars.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_schema.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_sparse_tensor.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_strategies.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_substrait.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_table.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_tensor.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_types.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_udf.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_util.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/test_without_numpy.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/util.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/wsgi_examples.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/feather\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/feather/v0.17.0.version.2-compression.lz4.feather -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/feather\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/README.md -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/orc/decimal.orc -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/orc\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/data/parquet\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/test_conversion.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/interchange\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/interchange/test_interchange_spec.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/interchange\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/common.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/conftest.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/encryption.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_basic.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_compliant_nested_type.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_data_types.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_dataset.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_datetime.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_encryption.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_metadata.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_pandas.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.linux-x86_64-cpython-39/pyarrow/tests/parquet\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-39/pyarrow/vendored\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/vendored/__init__.py -> build/lib.linux-x86_64-cpython-39/pyarrow/vendored\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/vendored/docscrape.py -> build/lib.linux-x86_64-cpython-39/pyarrow/vendored\n",
      "  \u001b[31m   \u001b[0m copying pyarrow/vendored/version.py -> build/lib.linux-x86_64-cpython-39/pyarrow/vendored\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating /mnt/tmp/pip-install-0zoh_iee/pyarrow_c1c8a6d32744407c954207e4ee3db841/build/temp.linux-x86_64-cpython-39\n",
      "  \u001b[31m   \u001b[0m -- Running cmake for PyArrow\n",
      "  \u001b[31m   \u001b[0m cmake -DCMAKE_INSTALL_PREFIX=/mnt/tmp/pip-install-0zoh_iee/pyarrow_c1c8a6d32744407c954207e4ee3db841/build/lib.linux-x86_64-cpython-39/pyarrow -DPYTHON_EXECUTABLE=/mnt/notebook-env/bin/python -DPython3_EXECUTABLE=/mnt/notebook-env/bin/python -DPYARROW_CXXFLAGS= -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_CYTHON_CPP=off -DPYARROW_GENERATE_COVERAGE=off -DCMAKE_BUILD_TYPE=release /mnt/tmp/pip-install-0zoh_iee/pyarrow_c1c8a6d32744407c954207e4ee3db841\n",
      "  \u001b[31m   \u001b[0m error: command 'cmake' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pyarrow\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hSuccessfully built pyspark\n",
      "Failed to build pyarrow\n",
      "\u001b[31mERROR: Could not build wheels for pyarrow, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.4 datasets==3.3.2 pandas==1.5.3 matplotlib==3.8.4 seaborn==0.13.2 numpy==1.26.4 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:14:06.326381Z",
     "iopub.status.busy": "2025-07-11T16:14:06.326070Z",
     "iopub.status.idle": "2025-07-11T16:15:24.462073Z",
     "shell.execute_reply": "2025-07-11T16:15:24.461393Z",
     "shell.execute_reply.started": "2025-07-11T16:14:06.326351Z"
    },
    "id": "SGfM4jPrMqer"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc00773ae73541a0969fd19dc975d399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1752833681866_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-79-58.ec2.internal:20888/proxy/application_1752833681866_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-27QC2DU3ZBCQL\n",
       "\" application-id=\"application_1752833681866_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-79-58.ec2.internal:8042/node/containerlogs/container_1752833681866_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Behavior Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load the paths to the datasets/csv files\n",
    "amazon_purchases_path = \"s3a://customeranalysis123/amazon-purchases.csv\"\n",
    "survey_path = \"s3a://customeranalysis123/survey.csv\"\n",
    "fields_path = \"s3a://customeranalysis123/fields.csv\"\n",
    "\n",
    "# Load datasets into PySpark DataFrames\n",
    "amazon_purchases = spark.read.csv(amazon_purchases_path, header=True, inferSchema=True)\n",
    "survey = spark.read.csv(survey_path, header=True, inferSchema=True)\n",
    "fields = spark.read.csv(fields_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c83fe947b1c4f079318c47905d0dd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = spark.sparkContext # access SparkContext from SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771293594be4d51bb661a61f1b486da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Collecting pandas>=0.25\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib64/python3.7/site-packages (from seaborn) (1.20.0)\n",
      "Collecting typing_extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting matplotlib!=3.6.1,>=3.1\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.25->seaborn) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas, typing-extensions, packaging, pyparsing, cycler, kiwisolver, pillow, fonttools, matplotlib, seaborn\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.5 matplotlib-3.5.3 packaging-24.0 pandas-1.3.5 pillow-9.5.0 pyparsing-3.1.4 python-dateutil-2.9.0.post0 seaborn-0.12.2 typing-extensions-4.7.1\n",
      "\n",
      "Requirement already satisfied: matplotlib in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib64/python3.7/site-packages (from matplotlib) (1.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib64/python3.7/site-packages (from scikit-learn) (1.20.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib64/python3.7/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: scipy, threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0\n",
      "\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.33.13-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3) (1.0.1)\n",
      "Collecting botocore<1.34.0,>=1.33.13\n",
      "  Downloading botocore-1.33.13-py3-none-any.whl (11.8 MB)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version < \"3.10\"\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages (from botocore<1.34.0,>=1.33.13->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.13->boto3) (1.13.0)\n",
      "Installing collected packages: urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.33.13 botocore-1.33.13 s3transfer-0.8.2 urllib3-1.26.20\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"seaborn\")\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"scikit-learn\")\n",
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e84e8a8ba4d7da2f3bc006776515f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/yarn/usercache/livy/appcache/application_1752833681866_0001/container_1752833681866_0001_01_000001/tmp/spark-85247941-4a89-4e49-a3ee-be51a1062bc5/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c255f64b8943c683a18df6cf0ad651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Date: date (nullable = true)\n",
      " |-- Purchase Price Per Unit: double (nullable = true)\n",
      " |-- Quantity: double (nullable = true)\n",
      " |-- Shipping Address State: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ASIN/ISBN (Product Code): string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Survey ResponseID: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Survey ResponseID: string (nullable = true)\n",
      " |-- Q-demos-age: string (nullable = true)\n",
      " |-- Q-demos-hispanic: string (nullable = true)\n",
      " |-- Q-demos-race: string (nullable = true)\n",
      " |-- Q-demos-education: string (nullable = true)\n",
      " |-- Q-demos-income: string (nullable = true)\n",
      " |-- Q-demos-gender: string (nullable = true)\n",
      " |-- Q-sexual-orientation: string (nullable = true)\n",
      " |-- Q-demos-state: string (nullable = true)\n",
      " |-- Q-amazon-use-howmany: string (nullable = true)\n",
      " |-- Q-amazon-use-hh-size: string (nullable = true)\n",
      " |-- Q-amazon-use-how-oft: string (nullable = true)\n",
      " |-- Q-substance-use-cigarettes: string (nullable = true)\n",
      " |-- Q-substance-use-marijuana: string (nullable = true)\n",
      " |-- Q-substance-use-alcohol: string (nullable = true)\n",
      " |-- Q-personal-diabetes: string (nullable = true)\n",
      " |-- Q-personal-wheelchair: string (nullable = true)\n",
      " |-- Q-life-changes: string (nullable = true)\n",
      " |-- Q-sell-YOUR-data: string (nullable = true)\n",
      " |-- Q-sell-consumer-data: string (nullable = true)\n",
      " |-- Q-small-biz-use: string (nullable = true)\n",
      " |-- Q-census-use: string (nullable = true)\n",
      " |-- Q-research-society: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- fields: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "amazon_purchases.printSchema()\n",
    "survey.printSchema()\n",
    "fields.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d606aafba554deb920fdff35b4987b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "amazon_purchases = amazon_purchases.withColumnRenamed(\"Survey ResponseID\", \"response_id\") \\\n",
    "                                   .withColumnRenamed(\"Field ID\", \"field_id\")\n",
    "survey = survey.withColumnRenamed(\"Survey ResponseID\", \"response_id\")\n",
    "fields = fields.withColumnRenamed(\"Field ID\", \"field_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:33:56.538425Z",
     "iopub.status.busy": "2025-07-11T16:33:56.538108Z",
     "iopub.status.idle": "2025-07-11T16:34:39.634472Z",
     "shell.execute_reply": "2025-07-11T16:34:39.633540Z",
     "shell.execute_reply.started": "2025-07-11T16:33:56.538397Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808d735420854d04b27f63a319a4a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+-------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "|      response_id|Order Date|Purchase Price Per Unit|Quantity|Shipping Address State|               Title|ASIN/ISBN (Product Code)|     Category|  Q-demos-age|Q-demos-hispanic|        Q-demos-race|Q-demos-education|   Q-demos-income|Q-demos-gender|Q-sexual-orientation|Q-demos-state|Q-amazon-use-howmany|Q-amazon-use-hh-size|Q-amazon-use-how-oft|Q-substance-use-cigarettes|Q-substance-use-marijuana|Q-substance-use-alcohol|Q-personal-diabetes|Q-personal-wheelchair|Q-life-changes|    Q-sell-YOUR-data|Q-sell-consumer-data|Q-small-biz-use|Q-census-use|Q-research-society|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+-------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "|R_01vNIayewjIIKMF|2018-12-04|                   7.98|     1.0|                    NJ|SanDisk Ultra 16G...|              B0143RTB1E| FLASH_MEMORY|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          null|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-22|                  13.99|     1.0|                    NJ|Betron BS10 Earph...|              B01MA1MJ6H|   HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          null|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-24|                   8.99|     1.0|                    NJ|                null|              B078JZTFN3|         null|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          null|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-25|                  10.45|     1.0|                    NJ|Perfecto Stainles...|              B06XWF9HML|DISHWARE_BOWL|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          null|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-25|                   10.0|     1.0|                    NJ|Proraso Shaving C...|              B00837ZOI0|SHAVING_AGENT|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          null|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+-------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Merge the datasets\n",
    "merged_data =  amazon_purchases\\\n",
    "    .join(survey, on=\"response_id\",how=\"inner\")\n",
    "\n",
    "# Display the merged data\n",
    "merged_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgFaXMsoO04U"
   },
   "source": [
    "#1. Data Preparation\n",
    "\n",
    "Before analysis, the data needs to be prepared to ensure consistency and efficiency.\n",
    "- Check for data consistency and ensure all columns are correctly formatted.\n",
    "- Structure and prepare the dataset for further processing, ensuring that relevant features are retained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L2neUeVP3f6t"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118a25d244ab4114ad61dc7d1b004e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+\n",
      "|response_id|Order Date|Purchase Price Per Unit|Quantity|Shipping Address State|Title|ASIN/ISBN (Product Code)|Category|Q-demos-age|Q-demos-hispanic|Q-demos-race|Q-demos-education|Q-demos-income|Q-demos-gender|Q-sexual-orientation|Q-demos-state|Q-amazon-use-howmany|Q-amazon-use-hh-size|Q-amazon-use-how-oft|Q-substance-use-cigarettes|Q-substance-use-marijuana|Q-substance-use-alcohol|Q-personal-diabetes|Q-personal-wheelchair|Q-life-changes|Q-sell-YOUR-data|Q-sell-consumer-data|Q-small-biz-use|Q-census-use|Q-research-society|\n",
      "+-----------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+\n",
      "|          0|         0|                      0|       0|                 86832|89740|                     951|   89435|          0|               0|           0|                0|             0|             0|                   0|            0|                   0|                   0|                   0|                         0|                        0|                      0|                  0|                    0|       1212958|               0|                   0|              0|           0|                 0|\n",
      "+-----------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, col\n",
    "\n",
    "# Check for missing values in the merged dataset\n",
    "null_counts=merged_data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in merged_data.columns])\n",
    "\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS3ZMXS-eNYE"
   },
   "source": [
    "#2. Data Cleaning <font color = red>[20 marks]</font> <br>\n",
    "\n",
    "Prepare the data for further analysis by performing data cleaning such as missing value treatment, handle data schema, outlier analysis, and relevant feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QanhKxhkC0zp"
   },
   "source": [
    "## 2.1 Handling Missing values <font color = red>[10 marks]</font> <br>\n",
    "Handle missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zrtZmZ4sR6oX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a9b22a3fb14ee2a8f732257694f02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns with missing (null) values (excluding already filled):\n",
      "Category:89435 null(s)\n",
      "Q-life-changes:1212958 null(s)\n",
      "Title:89740 null(s)\n",
      "ASIN/ISBN (Product Code):951 null(s)\n",
      "Shipping Address State:86832 null(s)"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "#  Fill missing (null) values with the appropriate techniques as required by the analysis\n",
    "# categorial column- fill with 'unknown'\n",
    "categorical_cols = ['Q-demos-gender','Q-demos-education','Q-demos-race']\n",
    "merged_data = merged_data.fillna('Unknown',subset=categorical_cols)\n",
    "\n",
    "numerical_cols = ['Q-demos-age','Q-demos-income','Quantity','Purchase Price Per Unit']\n",
    "merged_data = merged_data.fillna(0,subset=numerical_cols)\n",
    "\n",
    "# Aggregate and count missing values (nulls) for each column after replacement\n",
    "all_columns= merged_data.columns\n",
    "remaining_columns = list(set(all_columns)-set(categorical_cols)-set(numerical_cols))\n",
    "\n",
    "remaining_null_counts = merged_data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in remaining_columns])\n",
    "\n",
    "# Display the count of remaining missing values in each column\n",
    "\n",
    "null_dict = remaining_null_counts.first().asDict()\n",
    "print(\"Remaining columns with missing (null) values (excluding already filled):\")\n",
    "for col_name,count in null_dict.items():\n",
    "    if count>0:\n",
    "        print(f\"{col_name}:{count} null(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kHhSEkFiSANw"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cd0ef6196a44209431289d69712890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+----------+\n",
      "|Order Date|order_date_parsed|order_month|order_year|\n",
      "+----------+-----------------+-----------+----------+\n",
      "|2018-12-04|       2018-12-04|         12|      2018|\n",
      "|2018-12-22|       2018-12-22|         12|      2018|\n",
      "|2018-12-24|       2018-12-24|         12|      2018|\n",
      "|2018-12-25|       2018-12-25|         12|      2018|\n",
      "|2018-12-25|       2018-12-25|         12|      2018|\n",
      "|2019-02-18|       2019-02-18|          2|      2019|\n",
      "|2019-02-18|       2019-02-18|          2|      2019|\n",
      "|2019-04-23|       2019-04-23|          4|      2019|\n",
      "|2019-04-23|       2019-04-23|          4|      2019|\n",
      "|2019-05-02|       2019-05-02|          5|      2019|\n",
      "+----------+-----------------+-----------+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, month, year, to_date\n",
    "\n",
    "# Perform appropriate feature engineering. Eg. Extract order date, month, year and cast to the appropriate values\n",
    "# 1 convert order date from string to datetype\n",
    "merged_data = merged_data.withColumn(\"order_date_parsed\",to_date(col(\"Order Date\"),\"MM/dd/yyyy\"))\n",
    "# 2 Extract Month and Year\n",
    "\n",
    "merged_data = merged_data.withColumn(\"order_month\",month(col(\"order_date_parsed\")))\\\n",
    ".withColumn(\"order_year\",year(col(\"order_date_parsed\")))\n",
    "\n",
    "# Display the updated dataset\n",
    "merged_data.select(\"Order Date\",\"order_date_parsed\",\"order_month\",\"order_year\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WcD1wyX9SKPs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6534d785accd4a9e993a5c248b499bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+--------------+---------------------+\n",
      "|   Q-demos-income|Q-demos-income-mapped|Q-demos-gender|Q-demos-gender-mapped|\n",
      "+-----------------+---------------------+--------------+---------------------+\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "|$25,000 - $49,999|                    1|          Male|                    0|\n",
      "+-----------------+---------------------+--------------+---------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "# Map categorical income to numerical values\n",
    "income_mapping = {\n",
    "    'Less than $25,000': 0,\n",
    "    '$25,000 - $49,999': 1,\n",
    "    '$50,000 - $74,999': 2,\n",
    "    '$75,000 - $99,999': 3,\n",
    "    '$100,000 - $149,999': 4,\n",
    "    '$150,000 or more': 5\n",
    "}\n",
    "\n",
    "income_map_expr = create_map([lit(x) for x in chain (*income_mapping.items())])\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Q-demos-income-mapped\",income_map_expr[col(\"Q-demos-income\")])\n",
    "\n",
    "# Map gender to numerical values\n",
    "\n",
    "gender_mapping = {\n",
    "    'Male' : 0,\n",
    "    'Female':1,\n",
    "    'Non-Binary':2,\n",
    "    'Unknown':-1\n",
    "}\n",
    "\n",
    "gender_map_expr = create_map([lit(x) for x in chain (*gender_mapping.items())])\n",
    "merged_data = merged_data.withColumn(\"Q-demos-gender-mapped\",gender_map_expr[col(\"Q-demos-gender\")])\n",
    "\n",
    "# Display the updated dataset\n",
    "\n",
    "\n",
    "merged_data.select(\"Q-demos-income\",\"Q-demos-income-mapped\",\"Q-demos-gender\",\"Q-demos-gender-mapped\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdkrHeIKSTr7"
   },
   "source": [
    "## 2.3 Data Cleaning <font color = red>[5 marks]</font> <br>\n",
    "Handle data cleaning techniques such as data duplication, dropping unnecessary values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jq52EEBiSV74",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d60bc1015bf4869bee2d29a07b085c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicates: 11516\n",
      "Number of Duplicates After Cleaning: 0"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(\"Number of Duplicates:\", merged_data.count() - merged_data.dropDuplicates().count())\n",
    "\n",
    "\n",
    "# Remove duplicates\n",
    "merged_data = merged_data.dropDuplicates()\n",
    "\n",
    "\n",
    "# Verify duplicates after cleaning\n",
    "print(\"Number of Duplicates After Cleaning:\", merged_data.count() - merged_data.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c9njYfvDTvQN"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ffa3f6f62493fa48cc03b73761289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+----------------+-------------+----------------+------------------+--------------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------------+--------------------+--------------------+---------------+------------+------------------+-----------------+-----------+----------+---------------------+---------------------+\n",
      "|      response_id|Order Date|Purchase Price Per Unit|Quantity|Shipping Address State|               Title|ASIN/ISBN (Product Code)|        Category|  Q-demos-age|Q-demos-hispanic|      Q-demos-race|   Q-demos-education|   Q-demos-income|Q-demos-gender|Q-sexual-orientation|Q-demos-state|Q-amazon-use-howmany|Q-amazon-use-hh-size|Q-amazon-use-how-oft|Q-substance-use-cigarettes|Q-substance-use-marijuana|Q-substance-use-alcohol|Q-personal-diabetes|Q-personal-wheelchair|      Q-life-changes|    Q-sell-YOUR-data|Q-sell-consumer-data|Q-small-biz-use|Q-census-use|Q-research-society|order_date_parsed|order_month|order_year|Q-demos-income-mapped|Q-demos-gender-mapped|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+----------------+-------------+----------------+------------------+--------------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------------+--------------------+--------------------+---------------+------------+------------------+-----------------+-----------+----------+---------------------+---------------------+\n",
      "|R_1k0Q9IrJcX82jAz|2022-03-22|                  19.99|     1.0|                    WA|K ROCKSHEAT Ceram...|              B07QX3SPB8|      BAKING_PAN|18 - 24 years|              No|             Asian|   Bachelor's degree|$50,000 - $74,999|        Female|              LGBTQ+|   Washington|        1 (just me!)|        1 (just me!)|5 - 10 times per ...|                        No|                       No|                     No|                 No|                   No|Lost a job ,Moved...|                  No|                  No|             No|          No|                No|       2022-03-22|          3|      2022|                    2|                    1|\n",
      "|R_1k0jNifwb1pjiE2|2020-11-22|                  19.97|     1.0|                    TX|Rocco & Roxie Sta...|              B00CKFL93K|    PET_SUPPLIES|35 - 44 years|              No|White or Caucasian|Graduate or profe...|$50,000 - $74,999|          Male|heterosexual (str...|        Texas|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|                null|                 Yes|                 Yes|            Yes|         Yes|               Yes|       2020-11-22|         11|      2020|                    2|                    0|\n",
      "|R_1k1zPs6FKsqm1Ib|2020-06-28|                   4.23|     1.0|                    FL|Reynolds Kitchens...|              B07F6F631N|    BAKING_PAPER|25 - 34 years|              No|White or Caucasian|   Bachelor's degree|$25,000 - $49,999|        Female|              LGBTQ+|      Florida|                   2|                   2|5 - 10 times per ...|                        No|                      Yes|                     No|                 No|                   No| Lost a job ,Divorce|Yes if I get part...|Yes if consumers ...|            Yes|         Yes|               Yes|       2020-06-28|          6|      2020|                    1|                    1|\n",
      "|R_1k1zPs6FKsqm1Ib|2022-09-15|                   4.99|     1.0|                    FL|Organic Pancake a...|              B00NY5ZW8I|      BAKING_MIX|25 - 34 years|              No|White or Caucasian|   Bachelor's degree|$25,000 - $49,999|        Female|              LGBTQ+|      Florida|                   2|                   2|5 - 10 times per ...|                        No|                      Yes|                     No|                 No|                   No| Lost a job ,Divorce|Yes if I get part...|Yes if consumers ...|            Yes|         Yes|               Yes|       2022-09-15|          9|      2022|                    1|                    1|\n",
      "|R_1kFTMrVOWJRG0PT|2021-02-27|                   20.0|     1.0|                    IA|Men’s Revitalizin...|              B07XVD5GVC|SKIN_MOISTURIZER|18 - 24 years|              No|White or Caucasian|   Bachelor's degree|$50,000 - $74,999|          Male|heterosexual (str...|    Minnesota|        1 (just me!)|                  4+|Less than 5 times...|                        No|                       No|                    Yes|                 No|                   No|Moved place of re...|                  No|                  No|            Yes|I don't know|               Yes|       2021-02-27|          2|      2021|                    2|                    0|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+----------------+-------------+----------------+------------------+--------------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------------+--------------------+--------------------+---------------+------------+------------------+-----------------+-----------+----------+---------------------+---------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "cleaned_data_path = \"s3a://customeranalysis123/cleaned_customer_data\"\n",
    "\n",
    "merged_data.write.csv(cleaned_data_path, header=True, mode='overwrite')\n",
    "\n",
    "# Load the cleaned dataset from the location\n",
    "cleaned_data = spark.read.csv(cleaned_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Cleaned Data:\")\n",
    "cleaned_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zc_-ALrVh5N"
   },
   "source": [
    "# 3. Exploratory Data Analysis <font color = red>[55 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vguqgj3uDOsN"
   },
   "source": [
    "## 3.1 Analyse purchases by hour, day and month <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Examine overall trends in purchases over time and analyse the trends by hour, day, month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "I2N2T4daX_bG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b3760ea09a4d6fa3c173bbd13e2bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Purchase Distribution by Hour, Day, and Month\n",
    "\n",
    "from pyspark.sql.functions import hour, dayofweek, month\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract hour, day, and month\n",
    "df_time = merged_data.withColumn(\"hour\",hour(\"order_date_parsed\"))\\\n",
    ".withColumn(\"day_of_week\",dayofweek(\"order_date_parsed\"))\\\n",
    ".withColumn(\"month\",month(\"order_date_parsed\"))\n",
    "\n",
    "\n",
    "\n",
    "# Group and count purchases by time factors\n",
    "\n",
    "hourly_count = df_time.groupBy(\"hour\").count().orderBy(\"hour\").toPandas()\n",
    "daily_count = df_time.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\").toPandas()\n",
    "monthly_count = df_time.groupBy(\"month\").count().orderBy(\"month\").toPandas()\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.barplot(x=\"hour\",y=\"count\",data=hourly_count,ax=axes[0])\n",
    "axes[0].set_title(\"Purchases By Hour\")\n",
    "\n",
    "sns.barplot(x=\"day_of_week\",y=\"count\",data=daily_count,ax=axes[1])\n",
    "axes[1].set_title(\"Purchases By Day of Week\")\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(['sun','Mon','Tues','Wed','Thu','Fri','Sat'])\n",
    "\n",
    "sns.barplot(x=\"month\",y=\"count\",data=monthly_count,ax=axes[2])\n",
    "axes[2].set_title(\"Purchases By Month\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"month_count.png\")\n",
    "s3.upload_file('month_count.png', 'customeranalysis123', 'month_count.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b022ed96ce834b4ab6dffe635eba12c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monthly Purchase Trends\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Extract month and year from 'Order Date'\n",
    "\n",
    "data_monthly = cleaned_data.withColumn(\"year_month\", date_format(\"order_date_parsed\", \"yyyy-MM\"))\n",
    "# Group by month and count purchases\n",
    "\n",
    "monthly_counts = data_monthly.groupBy(\"year_month\").count().orderBy(\"year_month\")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "monthly_counts_pandas = monthly_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.lineplot(x=\"year_month\", y=\"count\", data=monthly_counts_pandas, marker=\"o\", color=\"Blue\")\n",
    "\n",
    "plt.title(\"Monthly Purchase Trends\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Purchases\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"monthly_trend.png\")\n",
    "s3.upload_file('monthly_trend.png', 'customeranalysis123', 'monthly_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uPGP9f9pyawh"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32abdb90ff4089b5321b803046ae17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Yealy Purchase Trends\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Group by Year and count purchases\n",
    "data_yearly = cleaned_data.withColumn(\"order_year\", date_format(\"order_date_parsed\", \"yyyy\"))\n",
    "yearly_counts = data_yearly.groupBy(\"order_year\").count().orderBy(\"order_year\")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "yearly_counts_pandas = yearly_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=\"order_year\", y=\"count\", data=yearly_counts_pandas)\n",
    "\n",
    "plt.title(\"Yearly Purchase Trends\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total  Purchases\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"yearly_trend.png\")\n",
    "s3.upload_file('yearly_trend.png', 'customeranalysis123', 'yearly_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TckpboEbDTfJ"
   },
   "source": [
    "## 3.2 Customer Demographics vs Purchase Frequency <font color = red>[5 marks]</font> <br>\n",
    "Analyse the trends between the customer deographics and the purchase frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XMfGJhPlX_Yv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cf630b08634b5c8f91ca9425a1ca6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation Between Demographics and Purchase Frequency\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by demographic attributes and count purchases\n",
    "gender_counts = cleaned_data.groupBy(\"Q-demos-gender\").count()\n",
    "\n",
    "age_counts = cleaned_data.groupBy(\"Q-demos-age\").count()\n",
    "income_counts = cleaned_data.groupBy(\"Q-demos-income\").count()\n",
    "state_counts = cleaned_data.groupBy(\"Q-demos-state\").count()\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "gender_counts_pd = gender_counts.toPandas()\n",
    "age_counts_pd = age_counts.toPandas()\n",
    "income_counts_pd = income_counts.toPandas()\n",
    "state_counts_pd = state_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "# Gender\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=gender_counts_pd, x=\"Q-demos-gender\", y=\"count\", palette=\"pastel\")\n",
    "plt.title(\"Purchases by Gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Number of Purchases\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gender_trend.png\")\n",
    "s3.upload_file('gender_trend.png', 'customeranalysis123', 'gender_trend.png') \n",
    "plt.show()\n",
    "\n",
    "#Age\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=age_counts_pd, x=\"Q-demos-age\", y=\"count\", palette=\"Set2\")\n",
    "plt.title(\"Purchases by Age Group\")\n",
    "plt.xlabel(\"Age Group\")\n",
    "plt.ylabel(\"Number of Purchases\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"age_trend.png\")\n",
    "s3.upload_file('age_trend.png', 'customeranalysis123', 'age_trend.png')\n",
    "plt.show()\n",
    "\n",
    "#income\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=income_counts_pd, x=\"Q-demos-income\", y=\"count\", palette=\"Blues_d\")\n",
    "plt.title(\"Purchases by Income \")\n",
    "plt.xlabel(\"Income Range\")\n",
    "plt.ylabel(\"Number of Purchases\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"income_trend.png\")\n",
    "s3.upload_file('income_trend.png', 'customeranalysis123', 'income_trend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXI_SKANDc8b"
   },
   "source": [
    "## 3.3 Purchase behavior weekend vs weekday <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Compare the purchase behavior of customer's on weekdays vs. weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iScn1AAZX_V_"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8217990b26042948eb18bf865674926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weekday vs. Weekend Purchase Behavior\n",
    "\n",
    "from pyspark.sql.functions import col,date_format, when, count,sum,avg,countDistinct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define weekdays and weekends\n",
    "\n",
    "merged_data = merged_data.withColumn(\"day_of_week\",date_format(\"order_date_parsed\",\"E\"))\\\n",
    ".withColumn(\"day_type\",when(col(\"day_of_week\").isin(\"Sat\",\"sun\"),\"Weekend\").otherwise(\"Weekday\"))\n",
    "\n",
    "\n",
    "# Group and count purchases\n",
    "\n",
    "summary_merged_data = merged_data.groupBy(\"day_type\").agg(count(\"*\").alias(\"total_transactions\"),\n",
    "                                                         sum(\"Purchase Price Per Unit\").alias(\"total_revenue\"),\n",
    "                                                         avg(\"Purchase Price Per Unit\").alias(\"average_purchase\"),\n",
    "                                                         countDistinct(\"response_id\").alias(\"unique_customers\"))\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "summary_pd = summary_merged_data.toPandas().sort_values(\"day_type\")\n",
    "\n",
    "# Plot for total_transaction,average_purchase and unique customer\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Total_transaction\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(summary_pd[\"day_type\"],summary_pd[\"total_transactions\"],color=[\"skyblue\",\"orange\"])\n",
    "plt.title(\"Total Transactions\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Average_purchase\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(summary_pd[\"day_type\"],summary_pd[\"average_purchase\"],color=[\"green\",\"red\"])\n",
    "plt.title(\"Average Purchase Amount\")\n",
    "plt.ylabel(\"Amount\")\n",
    "\n",
    "# Unique Customer\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(summary_pd[\"day_type\"],summary_pd[\"unique_customers\"],color=[\"purple\",\"pink\"])\n",
    "plt.title(\"Unique Customers\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.suptitle(\"Weekday Vs Weekend Purchase Behavior\",fontsize=16)\n",
    "plt.tight_layout\n",
    "plt.savefig('Weekday_Vs_Weekend.png')\n",
    "s3.upload_file('Weekday_Vs_Weekend.png', 'customeranalysis123', 'Weekday_Vs_Weekend.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ux5eQBRDgSs"
   },
   "source": [
    "## 3.4 Frequently purchased product pairs <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Analyze how frequently products are purchased together (also known as Market Basket Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PuAFZA93X_Tr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6a6c0a5c9d491ebde35fbd5ee79b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequently Purchased Product Pairs (Market Basket Analysis)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_set , col\n",
    "from itertools import combinations\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Group purchases by customer and collect items bought together\n",
    "customer_merged_data = merged_data.groupBy(\"response_id\").agg(collect_set(\"Title\").alias(\"items\"))\n",
    "\n",
    "# Explode item pairs\n",
    "\n",
    "pairs_rdd = customer_merged_data.rdd.flatMap(lambda row:[Row(item1=a,item2=b) for a,b in combinations(sorted(row['items'])[:10],2)])\n",
    "\n",
    "pair_df=spark.createDataFrame(pairs_rdd)\n",
    "\n",
    "\n",
    "# Count co-occurrences of items\n",
    "pair_counts = pair_df.groupBy(\"item1\",\"item2\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "top_pair_pd = pair_counts.limit(10).toPandas()\n",
    "\n",
    "# Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(\n",
    "y=[f\"{a} & {b}\" for a,b in zip(top_pair_pd[\"item1\"],top_pair_pd[\"item2\"])],width=top_pair_pd[\"count\"])\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.title(\"Top 10 Frequently Product Purchased Pair\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top_10_purchased_pair.png')\n",
    "s3.upload_file('Top_10_purchased_pair.png', 'customeranalysis123', 'Top_10_purchased_pair.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeD_yjU8DlEN"
   },
   "source": [
    "## 3.5 Examine Product Performance <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Examine the performance of products by calculating revenue and item popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Vr0fRcnSTvhE"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca5c0d631964db7a3d4d2ce3d55ce6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "# Contribution of Product Categories (Top 25)\n",
    "\n",
    "category_performance= merged_data.groupBy(\"Category\").agg(_sum(col(\"Purchase Price Per Unit\")* col(\"Quantity\")).alias(\"Total_Revenue\"),\n",
    "                                                         _sum(\"Quantity\").alias(\"Total_Quantity\"))\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_25_categories = category_performance.orderBy(col(\"Total_Revenue\").desc()).limit(25)\n",
    "top_25_pd= top_25_categories.toPandas()\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "top_25_pd[\"Category\"] = top_25_pd[\"Category\"].astype(str)\n",
    "plt.barh(top_25_pd[\"Category\"],top_25_pd[\"Total_Revenue\"],color='skyblue',label=\"Revenue\")\n",
    "plt.xlabel(\"Total_Revenue\")\n",
    "plt.title(\"Top 25 Product Category By Revenue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top_25_Category_Revenue.png')\n",
    "s3.upload_file('Top_25_Category_Revenue.png', 'customeranalysis123', 'Top_25_Category_Revenue.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9t7-ax4Dr7w"
   },
   "source": [
    "## 3.6 Top products by quantity <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Identify the most frequently purchased products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lnkrF7d8XAxt"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480054c8b2f4f9b96837a170dd4d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 Products by Quantity\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by product title and sum 'Quantity'\n",
    "top_products_qty= (\n",
    "    merged_data.groupBy(\"Title\")\n",
    "    .agg(_sum(\"Quantity\").alias(\"Total_Quantity\"))\n",
    "    .orderBy(col(\"Total_Quantity\").desc())\n",
    "    .limit(10) \n",
    ")\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "top_products_pd=top_products_qty.toPandas()\n",
    "\n",
    "top_products_pd[\"Title\"]= top_products_pd[\"Title\"].astype(str)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(top_products_pd[\"Title\"],top_products_pd[\"Total_Quantity\"],color='salmon')\n",
    "plt.xlabel(\"Total Quantity Sold\")\n",
    "plt.title(\"Top 10 Products By Quantity\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top_10_Products_by_Quantity.png')\n",
    "s3.upload_file('Top_10_Products_by_Quantity.png', 'customeranalysis123', 'Top_10_Products_by_Quantity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucWGxDLtDuH9"
   },
   "source": [
    "## 3.7 Distribution of Purchases by State <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Analyze the distribution of purchases across states and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "03-YzrUMXbpC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54371f7301f429998a35cb8209987f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of Purchases by State (Top 25)\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state_distribution= (\n",
    "    merged_data.groupBy(\"Shipping Address State\")\n",
    "    .agg(_sum(\"Quantity\").alias(\"Total_Quantity\"))\n",
    "    .orderBy(col(\"Total_Quantity\").desc())\n",
    "    .limit(25) \n",
    ")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "state_pd = state_distribution.toPandas()\n",
    "\n",
    "state_pd[\"Shipping Address State\"]= state_pd[\"Shipping Address State\"].astype(str)\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.barh(state_pd[\"Shipping Address State\"],state_pd[\"Total_Quantity\"],color='lightgreen')\n",
    "plt.xlabel(\"Total Quantity Purchased\")\n",
    "plt.title(\"Top 25 States By Purchase Quantity\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top_25_states_by_Quantity.png')\n",
    "s3.upload_file('Top_25_states_by_Quantity.png', 'customeranalysis123', 'Top_25_states_by_Quantity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKHw1EmGDwyo"
   },
   "source": [
    "## 3.8 Price vs Product Quantity <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Identify the Relationship between Price and Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "D0_KzBl2X6p6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646b3da7b5e34104b1e7328644e8e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relationship Between Price and Quantity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "price_qty_pd = merged_data.select(\"Purchase Price Per Unit\",\"Quantity\").toPandas()\n",
    "\n",
    "\n",
    "# removes rows with missing value\n",
    "\n",
    "price_qty_pd = price_qty_pd.dropna()\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(price_qty_pd[\"Purchase Price Per Unit\"],price_qty_pd[\"Quantity\"],alpha=0.5,color='purple')\n",
    "plt.xlabel(\"Purchase Price Per Unit\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.title(\"Relationship Between Price and Quantity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('Price_Vs_Quantity.png')\n",
    "s3.upload_file('Price_Vs_Quantity.png', 'customeranalysis123', 'Price_Vs_Quantity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a37zLfoyD39R"
   },
   "source": [
    "## 3.9 Analyse the spending KPIs <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "\n",
    "A popular KPI is average spend per customer. Calculate this metric as the ratio of total transaction amount from non-recurring payments divided by the total number of customers who made a purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U44bgKSgX6h6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebca2d791c214d57bbaeb841421059c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average Spend per Customer\n",
    "\n",
    "from pyspark.sql.functions import avg, col, sum as _sum, round\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_data= merged_data.withColumn(\"Total_Spend\",col(\"Purchase Price Per Unit\")*col(\"Quantity\"))\n",
    "\n",
    "# Group by customer and calculate average spend\n",
    "\n",
    "avg_spend_by_gender = merged_data.groupBy(\"Q-demos-gender\")\\\n",
    "     .agg(round(avg(\"Total_Spend\"),2).alias(\"Avg_Total_Spend\"))\\\n",
    "     .orderBy(\"Avg_Total_Spend\",ascending = False)\n",
    "\n",
    "        \n",
    "avg_spend_by_education= merged_data.groupBy(\"Q-demos-education\")\\\n",
    "     .agg(round(avg(\"Total_Spend\"),2).alias(\"Avg_Total_Spend\"))\\\n",
    "     .orderBy(\"Avg_Total_Spend\",ascending = False)\n",
    "        \n",
    "avg_spend_by_income= merged_data.groupBy(\"Q-demos-income\")\\\n",
    "     .agg(round(avg(\"Total_Spend\"),2).alias(\"Avg_Total_Spend\"))\\\n",
    "     .orderBy(\"Avg_Total_Spend\",ascending = False)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "avg_spend_pd = avg_spend_by_gender.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(avg_spend_pd[\"Q-demos-gender\"],avg_spend_pd[\"Avg_Total_Spend\"],color='mediumseagreen')\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Average Spend\")\n",
    "plt.title(\"Average Spend Per Customer By Gender\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('Average_Spend_Per_Customer_by_gender.png')\n",
    "s3.upload_file('Average_Spend_Per_Customer_by_gender.png', 'customeranalysis123', 'Average_Spend_Per_Customer_by_gender.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPMmGFtFPzdn"
   },
   "source": [
    "Analyse the Repeat Purchase Behavior of Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "evh5cQ2gX6ai"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b99d9f78ee9480d83d8968a810596de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Repeat Purchase Analysis Behavior Per Customers\n",
    "from pyspark.sql.functions import count, col, when, avg, round\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Calculate Total Spend per row\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\",\n",
    "    col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "# Step 2: Count how many purchases each customer made\n",
    "purchase_counts = merged_data.groupBy(\"response_id\")\\\n",
    "    .agg(count(\"*\").alias(\"num_purchases\"))\n",
    "\n",
    "# Step 3: Label each customer as Repeat or One-time buyer\n",
    "purchase_counts = purchase_counts.withColumn(\n",
    "    \"purchase_type\",\n",
    "    when(col(\"num_purchases\") > 1, \"Repeat\").otherwise(\"One-time\")\n",
    ")\n",
    "\n",
    "# Step 4: Join this info back to main data\n",
    "merged_with_type = merged_data.join(purchase_counts, on=\"response_id\", how=\"inner\")\n",
    "\n",
    "# Step 5: Group by purchase_type to get count of customers and average spend\n",
    "repeat_stats = merged_with_type.groupBy(\"purchase_type\") \\\n",
    "    .agg(\n",
    "        count(\"response_id\").alias(\"num_customers\"),\n",
    "        round(avg(\"Total_Spend\"), 2).alias(\"avg_spend\")\n",
    "    )\n",
    "\n",
    "# Step 6: Convert to Pandas for visualization\n",
    "repeat_stats_pd = repeat_stats.toPandas()\n",
    "\n",
    "# === Plot 1: Distribution of Repeat vs One-time Customers ===\n",
    "plt.bar(repeat_stats_pd[\"purchase_type\"], repeat_stats_pd[\"num_customers\"], color='steelblue')\n",
    "plt.title(\"Customer Distribution by Purchase Behavior\")\n",
    "plt.xlabel(\"Purchase Type\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('Customer_Distribution_Purchase_Behavior.png')\n",
    "s3.upload_file('Customer_Distribution_Purchase_Behavior.png', 'customeranalysis123', 'Customer_Distribution_Purchase_Behavior.png')\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: Average Spend by Purchase Type ===\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(repeat_stats_pd[\"purchase_type\"], repeat_stats_pd[\"avg_spend\"], color='seagreen')\n",
    "plt.title(\"Average Spend: Repeat vs One-time Buyers\")\n",
    "plt.xlabel(\"Purchase Type\")\n",
    "plt.ylabel(\"Average Spend ($)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('Average_Spend_By_Purchase_Type.png')\n",
    "s3.upload_file('Average_Spend_By_Purchase_Type.png', 'customeranalysis123', 'Average_Spend_By_Purchase_Type.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcoJFv8TP7ZJ"
   },
   "source": [
    "Analyse the top 10 high-engagement customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tRHnpItLX6KG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e2bddceb7146629249ebd49193b88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 High-Engagement Customers\n",
    "\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Calculate Total Spend column\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\",\n",
    "    col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate total spend per customer\n",
    "customer_spend = merged_data.groupBy(\"response_id\") \\\n",
    "    .agg(\n",
    "        spark_sum(\"Total_Spend\").alias(\"total_spend\")\n",
    "    )\n",
    "\n",
    "# Step 3: Get Top 10 High-Spending Customers\n",
    "top_customers = customer_spend.orderBy(col(\"total_spend\").desc()).limit(10)\n",
    "\n",
    "# Step 4: Convert to Pandas for plotting\n",
    "top_customers_pd = top_customers.toPandas()\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_customers_pd[\"response_id\"].astype(str), top_customers_pd[\"total_spend\"], color='darkorange')\n",
    "plt.xlabel(\"Total Spend ($)\")\n",
    "plt.ylabel(\"Customer ID (response_id)\")\n",
    "plt.title(\"Top 10 High-Engagement Customers by Total Spend\")\n",
    "plt.gca().invert_yaxis()  # Highest spender on top\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Top_10_High_Engagement_Customers.png\")\n",
    "s3.upload_file(\"Top_10_High_Engagement_Customers.png\", 'customeranalysis123', 'Top_10_High_Engagement_Customers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MccaRxN8EAt6"
   },
   "source": [
    "## 3.10 Seasonal trends in product purchases and their impact on revenues <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Investigate the seasonal trends in product purchases and their impact on the overall revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lVQf40UghaOZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6081fe86fac244e28aea3671bfc50fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seasonal Trends in Product Purchases and Their Impact on Revenue\n",
    "\n",
    "from pyspark.sql.functions import year, month, col, sum as spark_sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\",\n",
    "    col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "#  Extract year and month\n",
    "merged_data = merged_data.withColumn(\"year\", year(col(\"Order Date\"))) \\\n",
    "                         .withColumn(\"month\", month(col(\"Order Date\")))\n",
    "\n",
    "# Group by year and month, summing total revenue\n",
    "monthly_revenue = merged_data.groupBy(\"year\", \"month\") \\\n",
    "    .agg(spark_sum(\"Total_Spend\").alias(\"monthly_revenue\")) \\\n",
    "    .orderBy(\"year\", \"month\")\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "monthly_revenue_pd = monthly_revenue.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for yr in sorted(monthly_revenue_pd[\"year\"].unique()):\n",
    "    data = monthly_revenue_pd[monthly_revenue_pd[\"year\"] == yr]\n",
    "    plt.plot(data[\"month\"], data[\"monthly_revenue\"], marker='o', label=str(yr))\n",
    "\n",
    "plt.title(\"Seasonal Trends in Monthly Revenue\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Revenue ($)\")\n",
    "plt.xticks(range(1, 13))\n",
    "plt.legend(title=\"Year\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Seasonal_Trends_Revenue.png\")\n",
    "s3.upload_file(\"Seasonal_Trends_Revenue.png\", 'customeranalysis123', 'Seasonal_Trends_Revenue.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAw6x6V0EFSZ"
   },
   "source": [
    "## 3.11 Customer location vs purchasing behavior <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Examine the relationship between customer's location and their purchasing behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TCH3Uzt7hjBw"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2743826e8b14b7699981fd1d696d78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\", \n",
    "    col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "\n",
    "# Relationship Between Customer Location and Purchase Behavior\n",
    "\n",
    "# Group purchases by state and total spend\n",
    "\n",
    "location_spend = merged_data.groupBy(\"Q-demos-state\") \\\n",
    "    .agg(spark_sum(\"Total_Spend\").alias(\"Total_Revenue\")) \\\n",
    "    .orderBy(col(\"Total_Revenue\").desc())\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "location_spend_pd = location_spend.toPandas()\n",
    "\n",
    "# Plot revenue by state\n",
    "\n",
    "top_states = location_spend_pd.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_states[\"Q-demos-state\"], top_states[\"Total_Revenue\"], color=\"slateblue\")\n",
    "plt.xlabel(\"Total Revenue ($)\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.title(\"Top 10 States by Total Purchase Revenue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Revenue_By_Location.png\")\n",
    "s3.upload_file(\"Revenue_By_Location.png\", \"customeranalysis123\", \"Revenue_By_Location.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAI9P9w3bHzd"
   },
   "source": [
    "#4. Customer Segmentation and Insights <font color = red>[45 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmwsPLAScx70"
   },
   "source": [
    "## 4.1 Perform RFM Analysis <font color = red>[10 marks]</font> <br>\n",
    "\n",
    "RFM Analysis is a powerful customer segmentation technique used to evaluate and quantify customer value based on three key dimensions:\n",
    "- **Recency**,\n",
    "- **Frequency**,\n",
    "- **Monetary**.\n",
    "\n",
    "This method is particularly effective in identifying high-value customers, optimizing marketing strategies, and improving customer retention in the e-commerce industry.\n",
    "\n",
    "\n",
    "### 1. Recency (R)\n",
    "Recency measures how recently a customer made a purchase Customers who have purchased more recently are more likely to respond to promotions and offers.\n",
    "- **Application:** By ranking customers based on the number of days since their last transaction, you can prioritize those who are most engaged.\n",
    "\n",
    "### 2. Frequency (F)\n",
    "Frequency counts the number of purchases a customer has made over a given period.\n",
    "Frequent purchasers tend to be more loyal and are often a source of recurring revenue.\n",
    "- **Application:** Analyzing purchase frequency helps in identifying consistent buyers and understanding their buying patterns.\n",
    "\n",
    "### 3. Monetary (M)\n",
    "Monetary value represents the total amount of money a customer has spent.\n",
    "Customers who spend more are often more profitable, making them ideal targets for retention and upsell strategies.\n",
    "- **Application:** By assessing the monetary contribution, you can distinguish between high-value and low-value customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql_svp5od2e6"
   },
   "source": [
    "### Prepare data for RFM Analysis <font color = red>[2 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6ArqthBSaAG7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bfacb205f04dbab3ee11c62eae8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, max as spark_max, count, sum as spark_sum, col, lit, to_date\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Order Date\", to_date(col(\"Order Date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\", col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "# Get the latest order date in the dataset\n",
    "\n",
    "max_date = merged_data.agg(spark_max(\"Order Date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# Calculate RFM metrics\n",
    "\n",
    "rfm_df = merged_data.groupBy(\"response_id\").agg(\n",
    "    datediff(lit(max_date), spark_max(\"Order Date\")).alias(\"Recency\"),\n",
    "    count(\"*\").alias(\"Frequency\"),\n",
    "    spark_sum(\"Total_Spend\").alias(\"Monetary\")\n",
    ")\n",
    "\n",
    "# Filter out customers with no purchases\n",
    "rfm_df = rfm_df.filter(col(\"Monetary\") > 0)\n",
    "\n",
    "# Show RFM data\n",
    "\n",
    "rfm_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qIVvr5LUcz1m"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6638cc2cdd5f4c3bbdf87e759bcd7d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Recency_scaled  Frequency_scaled  Monetary_scaled\n",
      "0       -0.139780          0.482647         0.398730\n",
      "1       -0.622933         -0.556361        -0.258022\n",
      "2       -0.407676          0.261067         0.329777\n",
      "3       -0.709675          1.518122         1.430251\n",
      "4       -0.797902         -0.211111        -0.090658"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log1p\n",
    "import pandas as pd\n",
    "\n",
    "# Apply log transformation to skewed features\n",
    "\n",
    "rfm_log = rfm_df.withColumn(\"log_Recency\", log1p(col(\"Recency\"))) \\\n",
    "                .withColumn(\"log_Frequency\", log1p(col(\"Frequency\"))) \\\n",
    "                .withColumn(\"log_Monetary\", log1p(col(\"Monetary\")))\n",
    "\n",
    "# Convert to Pandas DataFrame (for scikit-learn compatibility)\n",
    "\n",
    "rfm_pd = rfm_log.select(\"log_Recency\", \"log_Frequency\", \"log_Monetary\").toPandas()\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_pd)\n",
    "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=[\"Recency_scaled\", \"Frequency_scaled\", \"Monetary_scaled\"])\n",
    "rfm_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "a-pcKc1fdilX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be4dcd7a2784758906b0dbac0ad6525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "\n",
    "# Calculate the Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)\n",
    "    kmeans.fit(rfm_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve with the number of clusters on the x-axis and WCSS on the y-axis\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--', color='teal')\n",
    "plt.title('Elbow Method - Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"elbow_curve_rfm.png\")\n",
    "s3.upload_file(\"elbow_curve_rfm.png\", \"customeranalysis123\", \"elbow_curve_rfm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JbcpAg2Od-pH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da34936a13e492ab2191c7cdf8546c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log_Recency  log_Frequency  log_Monetary  Cluster\n",
      "0     6.383507       5.866468      8.929807        0\n",
      "1     6.289716       4.574711      8.098199        0\n",
      "2     6.331502       5.590987      8.842496        0\n",
      "3     6.272877       7.153834     10.235964        2\n",
      "4     6.255750       5.003946      8.310122        0\n",
      "+------------------+------------------+------------------+-------+\n",
      "|log_Recency       |log_Frequency     |log_Monetary      |Cluster|\n",
      "+------------------+------------------+------------------+-------+\n",
      "|6.3835066348840055|5.8664680569332965|8.929807350718853 |0      |\n",
      "|6.289715570908998 |4.574710978503383 |8.098198976267856 |0      |\n",
      "|6.331501849893691 |5.5909869805108565|8.842496140272365 |0      |\n",
      "|6.272877006546167 |7.153833801578843 |10.235963959309123|2      |\n",
      "|6.255750041753367 |5.003946305945459 |8.310122280422137 |0      |\n",
      "|6.459904454377535 |5.934894195619588 |8.745569428320573 |0      |\n",
      "|6.285998094508865 |6.246106765481563 |9.152696428521098 |2      |\n",
      "|6.270988431858299 |6.1675164908883415|9.524450866701823 |2      |\n",
      "|6.418364935936212 |5.225746673713202 |8.172954876723969 |0      |\n",
      "|6.470799503782602 |5.043425116919247 |8.335863142915438 |0      |\n",
      "+------------------+------------------+------------------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the K-Means model using the optimal number of clusters obtained after understanding the elblow plot\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "rfm_pd['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "print(rfm_pd.head())\n",
    "\n",
    "# Add the assigned cluster labels to the Pandas DataFrame and convert back to PySpark if needed\n",
    "\n",
    "rfm_clustered_spark = spark.createDataFrame(rfm_pd)\n",
    "\n",
    "rfm_clustered_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6ZYNRsEDeHSC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f099a3fa1f124ba19a578fdf55a70f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the full RFM dataset from PySpark DataFrame to Pandas DataFrame for visualisation\n",
    "\n",
    "rfm_pd = rfm_df.select(\"Recency\", \"Frequency\", \"Monetary\").toPandas()\n",
    "\n",
    "# Log transformation to reduce skewness\n",
    "rfm_pd[\"Recency_log\"] = np.log1p(rfm_pd[\"Recency\"])\n",
    "rfm_pd[\"Frequency_log\"] = np.log1p(rfm_pd[\"Frequency\"])\n",
    "rfm_pd[\"Monetary_log\"] = np.log1p(rfm_pd[\"Monetary\"])\n",
    "\n",
    "# Generate a pairplot to visualise the relationships between the numeric RFM columns\n",
    "sns.pairplot(rfm_pd[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]], diag_kind=\"kde\")\n",
    "plt.suptitle(\"RFM Feature Relationships (Log Transformed)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"RFM_Pairplot.png\")\n",
    "s3.upload_file(\"RFM_Pairplot.png\", \"customeranalysis123\", \"RFM_Pairplot.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ND3vgs1h1kq"
   },
   "source": [
    "### Behavioral Trends Analysis <font color = red>[8 marks]</font> <br>\n",
    "\n",
    "Perform RFM analysis to study the behavior of customers to tailor marketing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "vpcIVLap8M7c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe6f14688b741de8f3db365b825d00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary PySpark functions for data processing\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, max as spark_max, datediff, countDistinct, sum as spark_sum, lit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group the dataset by 'Survey ResponseID' to calculate RFM (Recency, Frequency, Monetary) metrics\n",
    "merged_data = merged_data.withColumn(\"Order Date\", to_date(col(\"Order Date\"), \"yyyy-MM-dd\"))\n",
    "merged_data = merged_data.withColumn(\"Total_Spend\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "\n",
    "max_date = merged_data.agg(spark_max(\"Order Date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
    "\n",
    "# Compute 'Recency' as the difference between the latest date and the most recent order date\n",
    "\n",
    "# Compute 'Frequency' as the count of distinct product purchases (ASIN/ISBN)\n",
    "\n",
    "# Compute 'Monetary' as the total spending sum for each customer\n",
    "\n",
    "\n",
    "# By Assuming 'Title' is the product identifier instead of 'ASIN/ISBN'\n",
    "rfm_df = merged_data.groupBy(\"response_id\").agg(\n",
    "    datediff(lit(max_date), spark_max(\"Order Date\")).alias(\"Recency\"),\n",
    "    countDistinct(\"Title\").alias(\"Frequency\"),\n",
    "    spark_sum(\"Total_Spend\").alias(\"Monetary\")\n",
    ").filter(col(\"Monetary\") > 0)\n",
    "\n",
    "\n",
    "rfm_pd = rfm_df.toPandas()\n",
    "\n",
    "# Rename columns if required and normlise the distributions\n",
    "\n",
    "rfm_pd[\"Recency_log\"] = np.log1p(rfm_pd[\"Recency\"])\n",
    "rfm_pd[\"Frequency_log\"] = np.log1p(rfm_pd[\"Frequency\"])\n",
    "rfm_pd[\"Monetary_log\"] = np.log1p(rfm_pd[\"Monetary\"])\n",
    "\n",
    "# Convert the processed RFM dataset back to Pandas for sklearn compatibility for clustering\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_pd[[\"Recency_log\", \"Frequency_log\", \"Monetary_log\"]])\n",
    "\n",
    "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=[\"Recency_scaled\", \"Frequency_scaled\", \"Monetary_scaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nYk65PKiA9tY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55835de0b3be4460891ebdbad48449a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply K-Means clustering\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "\n",
    "# Fit the K-Means model and predict cluster labels for each customer\n",
    "\n",
    "rfm_scaled_df[\"Cluster\"] = kmeans.fit_predict(rfm_scaled_df)\n",
    "\n",
    "# Add the predicted cluster labels to the Pandas DataFrame\n",
    "\n",
    "rfm_pd[\"Cluster\"] = rfm_scaled_df[\"Cluster\"]\n",
    "\n",
    "# Convert the Pandas DataFrame back to a PySpark DataFrame\n",
    "\n",
    "rfm_spark_df = spark.createDataFrame(rfm_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t747TFBR5LE"
   },
   "source": [
    "Analyse the Cluster Distribution by Income <font color = red>[2 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d10d952a63d425ba4be577ce8207893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['response_id', 'Recency', 'Frequency', 'Monetary', 'Recency_log', 'Frequency_log', 'Monetary_log', 'Cluster']\n",
      "['response_id', 'Q-demos-age', 'Q-demos-hispanic', 'Q-demos-race', 'Q-demos-education', 'Q-demos-income', 'Q-demos-gender', 'Q-sexual-orientation', 'Q-demos-state', 'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft', 'Q-substance-use-cigarettes', 'Q-substance-use-marijuana', 'Q-substance-use-alcohol', 'Q-personal-diabetes', 'Q-personal-wheelchair', 'Q-life-changes', 'Q-sell-YOUR-data', 'Q-sell-consumer-data', 'Q-small-biz-use', 'Q-census-use', 'Q-research-society']"
     ]
    }
   ],
   "source": [
    "print(rfm_spark_df.columns)\n",
    "print(survey.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OOnJASHjBAen"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0ad298f4e049efb9a5ea5e4e728d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 1200x600 with 0 Axes>\n",
      "<AxesSubplot:xlabel='Q-demos-income', ylabel='Customer_Count'>\n",
      "Text(0.5, 1.0, 'Customer Cluster Distribution by Income Group')\n",
      "Text(0.5, 0, 'Income Group')\n",
      "Text(0, 0.5, 'Number of Customers')\n",
      "(array([0, 1, 2, 3, 4, 5, 6]), [Text(0, 0, '$75,000 - $99,999'), Text(1, 0, '$25,000 - $49,999'), Text(2, 0, '$150,000 or more'), Text(3, 0, 'Less than $25,000'), Text(4, 0, '$100,000 - $149,999'), Text(5, 0, '$50,000 - $74,999'), Text(6, 0, 'Prefer not to say')])"
     ]
    }
   ],
   "source": [
    "#Trend 1: Cluster Distribution by Income\n",
    "\n",
    "# Import the necessary function for counting records in PySpark\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Join the RFM dataset with the survey dataset using a common key\n",
    "\n",
    "survey_income = survey.select(\"response_id\", \"Q-demos-income\").dropDuplicates()\n",
    "\n",
    "rfm_with_income = rfm_spark_df.join(survey_income, on=\"response_id\", how=\"inner\")\n",
    "\n",
    "\n",
    "# Aggregate data to count the number of customers per Cluster-Income group\n",
    "\n",
    "cluster_income_counts = rfm_with_income.groupBy(\"Cluster\", \"Q-demos-income\").agg(\n",
    "    count(\"*\").alias(\"Customer_Count\")\n",
    ")\n",
    "\n",
    "# Convert the aggregated data from PySpark DataFrame to Pandas DataFrame for visualisation\n",
    "\n",
    "cluster_income_pd = cluster_income_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "if not cluster_income_pd.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=cluster_income_pd, x=\"Q-demos-income\", y=\"Customer_Count\", hue=\"Cluster\")\n",
    "    plt.title(\"Customer Cluster Distribution by Income Group\")\n",
    "    plt.xlabel(\"Income Group\")\n",
    "    plt.ylabel(\"Number of Customers\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Cluster_vs_Income.png\")\n",
    "    s3.upload_file(\"Cluster_vs_Income.png\", \"customeranalysis123\", \"Cluster_vs_Income.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available to plot. Check if join returned empty result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R56ntIlHR7LR"
   },
   "source": [
    "Analyse the Average Spending by Cluster <font color = red>[2 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "bHs8Z7xvh7eK"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b34b80a7f64d4ba54119a8db4855ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Trend 2: Average Spending by Cluster\n",
    "\n",
    "# Import the required function for calculating averages in PySpark\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute the average values of 'Recency_log', 'Frequency', and 'Monetary_log' for each customer cluster\n",
    "\n",
    "cluster_avg = rfm_spark_df.groupBy(\"Cluster\").agg(\n",
    "    avg(\"Recency_log\").alias(\"Avg_Recency_log\"),\n",
    "    avg(\"Frequency\").alias(\"Avg_Frequency\"),\n",
    "    avg(\"Monetary_log\").alias(\"Avg_Monetary_log\")\n",
    ")\n",
    "\n",
    "# Convert the aggregated cluster summary from PySpark DataFrame to Pandas DataFrame for visualisation\n",
    "\n",
    "cluster_avg_pd = cluster_avg.toPandas()\n",
    "\n",
    "# Generate a bar plot to visualise the average monetary spending per cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=cluster_avg_pd, x=\"Cluster\", y=\"Avg_Monetary_log\", palette=\"viridis\")\n",
    "plt.title(\"Average Monetary Spending by Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Average Log Monetary Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Avg_Monetary_Spending_by_Cluster.png\")\n",
    "s3.upload_file(\"Avg_Monetary_Spending_by_Cluster.png\", \"customeranalysis123\", \"Avg_Monetary_Spending_by_Cluster.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL2owKwoR8wi"
   },
   "source": [
    "Analyse the Purchase Frequency vs. Recency <font color = red>[2 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "UazBtjmAh7Vy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845a3c946b9346fd95c070e3abc4f81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<string>:11: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`)."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Trend 3: Purchase Frequency vs. Recency\n",
    "\n",
    "# Convert the RFM dataset from PySpark DataFrame to Pandas DataFrame for visualisation\n",
    "\n",
    "rfm_pd = rfm_spark_df.toPandas()\n",
    "\n",
    "# Generate a scatter plot to analyse the relationship between Purchase Frequency and Recency\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=rfm_pd, x=\"Recency\", y=\"Frequency\", hue=\"Cluster\", palette=\"deep\")\n",
    "plt.title(\"Purchase Frequency vs. Recency\")\n",
    "plt.xlabel(\"Recency (Days since Last Purchase)\")\n",
    "plt.ylabel(\"Purchase Frequency\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Frequency_vs_Recency.png\")\n",
    "s3.upload_file(\"Frequency_vs_Recency.png\", \"customeranalysis123\", \"Frequency_vs_Recency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLfJVjcWR_sB"
   },
   "source": [
    "Analyse the top categories by clusters <font color = red>[2 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0xbJU8B9h7Nj"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfab220d4ab4df1b5841bbb8b6d52de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 1200x600 with 0 Axes>\n",
      "<AxesSubplot:xlabel='Category', ylabel='Total_Spend'>\n",
      "Text(0.5, 1.0, 'Top 5 Spending Categories by Cluster')\n",
      "Text(0.5, 0, 'Category')\n",
      "Text(0, 0.5, 'Total Spending')\n",
      "(array([0, 1, 2, 3, 4, 5, 6]), [Text(0, 0, 'ABIS_BOOK'), Text(1, 0, 'GIFT_CARD'), Text(2, 0, 'HEADPHONES'), Text(3, 0, 'SHOES'), Text(4, 0, 'PERSONAL_COMPUTER'), Text(5, 0, 'PET_FOOD'), Text(6, 0, 'NUTRITIONAL_SUPPLEMENT')])"
     ]
    }
   ],
   "source": [
    "#Trend 4: Top Categories by Cluster\n",
    "\n",
    "# Import the necessary function to calculate the sum in PySpark\n",
    "\n",
    "from pyspark.sql.functions import sum as _sum, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Join the merged dataset with the RFM dataset to associate each customer with their respective cluster\n",
    "\n",
    "merged_with_cluster = merged_data.join(\n",
    "    rfm_spark_df.select(\"response_id\", \"Cluster\"),\n",
    "    on=\"response_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Group the filtered data by 'Category' and compute the total spending in each category\n",
    "category_cluster_spending = merged_with_cluster.groupBy(\"Cluster\", \"Category\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Spend\")\n",
    ")\n",
    "\n",
    "# Order the categories by total spending in descending order and select the top 5 highest spending categories\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Cluster\").orderBy(category_cluster_spending[\"Total_Spend\"].desc())\n",
    "\n",
    "top_categories = category_cluster_spending.withColumn(\n",
    "    \"rank\", row_number().over(windowSpec)\n",
    ").filter(\"rank <= 5\")\n",
    "\n",
    "# Convert the top categories dataset from PySpark DataFrame to Pandas DataFrame for visualisation\n",
    "\n",
    "top_categories_pd = top_categories.toPandas()\n",
    "\n",
    "top_categories_pd = top_categories_pd.sort_values(by=[\"Cluster\", \"Total_Spend\"], ascending=[True, False])\n",
    "\n",
    "\n",
    "# Plot the cluster\n",
    "if top_categories_pd.empty:\n",
    "    print(\"No top category data available to plot.\")\n",
    "else:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=top_categories_pd, x=\"Category\", y=\"Total_Spend\", hue=\"Cluster\")\n",
    "    plt.title(\"Top 5 Spending Categories by Cluster\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Total Spending\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Top_Categories_by_Cluster.png\")\n",
    "    s3.upload_file(\"Top_Categories_by_Cluster.png\", \"customeranalysis123\", \"Top_Categories_by_Cluster.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccbUK9OWlHXG"
   },
   "source": [
    "## 4.2 Insights <font color = red>[35 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSdWz7-tEbEn"
   },
   "source": [
    "### 4.2.1 When to schedule effective promotions. <font color = red>[3 marks]</font> <br>\n",
    "\n",
    "Compare sales across weekdays to schedule effective promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fb5c0353ec4d34a3afd3e503834adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['response_id', 'Order Date', 'Purchase Price Per Unit', 'Quantity', 'Shipping Address State', 'Title', 'ASIN/ISBN (Product Code)', 'Category', 'Q-demos-age', 'Q-demos-hispanic', 'Q-demos-race', 'Q-demos-education', 'Q-demos-income', 'Q-demos-gender', 'Q-sexual-orientation', 'Q-demos-state', 'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft', 'Q-substance-use-cigarettes', 'Q-substance-use-marijuana', 'Q-substance-use-alcohol', 'Q-personal-diabetes', 'Q-personal-wheelchair', 'Q-life-changes', 'Q-sell-YOUR-data', 'Q-sell-consumer-data', 'Q-small-biz-use', 'Q-census-use', 'Q-research-society', 'order_date_parsed', 'order_month', 'order_year', 'Q-demos-income-mapped', 'Q-demos-gender-mapped', 'day_of_week', 'day_type', 'Total_Spend', 'year', 'month']"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c5650e61a644d9aea3ee829ff39f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare sales across weekdays to schedule effective promotions\n",
    "from pyspark.sql.functions import dayofweek, sum as _sum, to_date, col, when, date_format\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract day of the week (1 = Sunday, 7 = Saturday)\n",
    "merged_data = merged_data.withColumn(\"order_date_parsed\", to_date(col(\"Order Date\"), \"MM/dd/yyyy\"))\n",
    "merged_data = merged_data.withColumn(\"Weekday\", dayofweek(\"order_date_parsed\"))\n",
    "\n",
    "# Step 2: Classify as 'Weekday' or 'Weekend'\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"day_type\",\n",
    "    when(col(\"Weekday\").isin(1, 7), \"Weekend\").otherwise(\"Weekday\")\n",
    ")\n",
    "\n",
    "# Group by weekday and sum total sales\n",
    "weekday_sales = merged_data.groupBy(\"Weekday\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Sales\")\n",
    ").orderBy(\"Weekday\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "weekday_sales_pd = weekday_sales.toPandas()\n",
    "\n",
    "# label\n",
    "weekday_labels = {\n",
    "    1: \"Sunday\", 2: \"Monday\", 3: \"Tuesday\", 4: \"Wednesday\",\n",
    "    5: \"Thursday\", 6: \"Friday\", 7: \"Saturday\"\n",
    "}\n",
    "weekday_sales_pd[\"Weekday_Name\"] = weekday_sales_pd[\"Weekday\"].map(weekday_labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=weekday_sales_pd, x=\"Weekday_Name\", y=\"Total_Sales\", order=[\n",
    "    \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"\n",
    "])\n",
    "plt.title(\"Total Sales by Weekday\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Sales_by_Weekday.png\")\n",
    "s3.upload_file(\"Sales_by_Weekday.png\", \"customeranalysis123\", \"Sales_by_Weekday.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_efTQMMEeGy"
   },
   "source": [
    "### 4.2.2 Top-selling Products <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Identify top-selling products by considering revenue and engagement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "YV0otvIEnJfa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bafc0140e24b26a0226e9d358d672f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<string>:32: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Identify top-selling products using revenue and engagement metrics\n",
    "\n",
    "# Group by product and sum revenue\n",
    "\n",
    "top_products = merged_data.groupBy(\"Title\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Revenue\")\n",
    ")\n",
    "\n",
    "# Get top 10 products by revenue\n",
    "\n",
    "top_10_products = top_products.orderBy(col(\"Total_Revenue\").desc()).limit(10)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "top_10_products_pd = top_10_products.toPandas()\n",
    "\n",
    "top_10_products_pd = top_10_products_pd.sort_values(by=\"Total_Revenue\", ascending=False)\n",
    "\n",
    "\n",
    "# Plot top products by revenue\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=top_10_products_pd, x=\"Total_Revenue\", y=\"Title\", palette=\"viridis\")\n",
    "plt.title(\"Top 10 Best-Selling Products by Revenue\")\n",
    "plt.xlabel(\"Total Revenue\")\n",
    "plt.ylabel(\"Product Title\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Top_10_Products_Revenue.png\")\n",
    "s3.upload_file(\"Top_10_Products_Revenue.png\", \"customeranalysis123\", \"Top_10_Products_Revenue.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqygUc8JEf7v"
   },
   "source": [
    "### 4.2.3 State-wise revenue Distribution <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Assess state-wise revenue to focus on high-growth areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yefqKQzOqiYI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce38ff5fb7b4113985920774d6db536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Assess state-wise revenue to focus on high-growth areas\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Total_Spend\", \n",
    "    col(\"Purchase Price Per Unit\") * col(\"Quantity\")\n",
    ")\n",
    "\n",
    "\n",
    "# Group by state and sum revenue\n",
    "state_revenue = merged_data.groupBy(\"Q-demos-state\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Revenue\")\n",
    ").withColumnRenamed(\"Q-demos-state\", \"State\")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "state_revenue_pd = state_revenue.toPandas().sort_values(by=\"Total_Revenue\", ascending=False)\n",
    "\n",
    "\n",
    "# Plot revenue by state\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=state_revenue_pd, x=\"State\", y=\"Total_Revenue\", palette=\"mako\")\n",
    "plt.title(\"Revenue by State\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Revenue_by_State.png\")\n",
    "s3.upload_file(\"Revenue_by_State.png\", \"customeranalysis123\", \"Revenue_by_State.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1ANO3B7EhYQ"
   },
   "source": [
    "### 4.2.4 Repeat Purchase Behavior <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Examine repeat purchase behavior to enhance retention initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "P1yhhY0Fr5TT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700a9e8e304b4f19bd72ee7db2dabbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|      response_id|total_purchases|\n",
      "+-----------------+---------------+\n",
      "|R_2xLvxRKX2448l9k|            352|\n",
      "|R_2eP9RaoDUn9qgIR|             96|\n",
      "|R_s4I8huB9BOcWI5H|            267|\n",
      "|R_1dKBjc0UTA0ZiYb|           1278|\n",
      "|R_10r2RIJOCZFOra8|            148|\n",
      "|R_2BaEdBiHdzJwaqI|            377|\n",
      "|R_2QrmzNCNpcA7B9p|            515|\n",
      "|R_3NFhCSnuJa54Jn3|            476|\n",
      "|R_30tLi6WqLJxToYS|            185|\n",
      "|R_OGUcDEj85m0paaR|            154|\n",
      "+-----------------+---------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#Examine repeat purchase behavior to enhance retention initiatives\n",
    "\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Count total purchases per customer\n",
    "\n",
    "customer_purchases = merged_data.groupBy(\"response_id\").agg(\n",
    "    count(\"*\").alias(\"total_purchases\")\n",
    ")\n",
    "\n",
    "# Filter for repeat customers (those with more than one purchase\n",
    "\n",
    "repeat_customers = customer_purchases.filter(col(\"total_purchases\") > 1)\n",
    "\n",
    "# Show sample data\n",
    "\n",
    "repeat_customers.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjMwmoHaEjCI"
   },
   "source": [
    "### 4.2.5 Flagging Potential Fraud <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Identify irregular transaction patterns to flag potential fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "4mvm_gMIsmDe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e97df483b6347468da914db08ec327d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----------+-------------+\n",
      "|      response_id|               Title|Total_Spend|Q-demos-state|\n",
      "+-----------------+--------------------+-----------+-------------+\n",
      "|R_1DonBwcSlvKQtt9|Acer Aspire 5 Sli...|     313.41|        Texas|\n",
      "|R_1FzkhYhOpeojsYt|ASUS AM4 TUF Gami...|      189.0|       Oregon|\n",
      "|R_1QLfbB3Lho273wG|Barbie DreamHouse...|      179.0|        Texas|\n",
      "|R_1eEsLZ6SYLsIG0s|2 Pcs 8 x 3 x 1 F...|     179.99|     New York|\n",
      "|R_1lgeraQD2mZ2SDP|Full-Automatic Wa...|     199.99|     Kentucky|\n",
      "|R_1M5mnTcAVcSRcli|                null|     205.56|     Kentucky|\n",
      "|R_1cYOJAbPFoatHTg|                null|     289.99|      Georgia|\n",
      "|R_1hMV4AHCk04WO5V|INSIGNIA 32-inch ...|     199.99|New Hampshire|\n",
      "|R_1i7Zoa74FWb4o89|VASAGLE TV Stand ...|     219.99|         Ohio|\n",
      "|R_1mkjLIewa5Obhi4|Paging Zone-Canon...|      499.0|     Missouri|\n",
      "+-----------------+--------------------+-----------+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#Identify irregular transaction patterns to flag potential fraud\n",
    "\n",
    "from pyspark.sql.functions import col, avg, stddev\n",
    "\n",
    "# Calculate the threshold for unusually high spendingstats = merged_data.select(avg(\"Total_Spend\").alias(\"mean_spend\"),\n",
    "stats = merged_data.select(\n",
    "    avg(\"Total_Spend\").alias(\"mean_spend\"),\n",
    "    stddev(\"Total_Spend\").alias(\"std_spend\")\n",
    ").collect()[0]\n",
    "\n",
    "mean_spend = stats[\"mean_spend\"]\n",
    "std_spend = stats[\"std_spend\"]\n",
    "\n",
    "# Consider spending to be unusually high if the total spent is greater than the mean + 3 * std dev\n",
    "\n",
    "threshold = mean_spend + 3 * std_spend\n",
    "\n",
    "# Filter transactions that exceed the threshold\n",
    "\n",
    "suspicious_transactions = merged_data.filter(col(\"Total_Spend\") > threshold)\n",
    "\n",
    "# Show suspicious transactions\n",
    "\n",
    "suspicious_transactions.select(\"response_id\", \"Title\", \"Total_Spend\", \"Q-demos-state\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELXXdmsPEkZ3"
   },
   "source": [
    "### 4.2.6 Demand Variations across product categories <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Perform inventory management by monitoring demand variations across product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "6XbTQ-sIuFt1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2b07f7c8d34d90b3aeea0ba0f0b4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Monitor demand variations across product categories (Top 25) for inventory management\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group by category and month, summing total revenue\n",
    "\n",
    "from pyspark.sql.functions import to_date, month, year\n",
    "\n",
    "merged_data = merged_data.withColumn(\"order_date_parsed\", to_date(col(\"Order Date\"), \"MM/dd/yyyy\"))\n",
    "merged_data = merged_data.withColumn(\"order_month\", month(col(\"order_date_parsed\")))\n",
    "merged_data = merged_data.withColumn(\"order_year\", year(col(\"order_date_parsed\")))\n",
    "\n",
    "\n",
    "# Compute total revenue per category\n",
    "\n",
    "category_trends = merged_data.groupBy(\"Category\", \"order_year\", \"order_month\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Revenue\")\n",
    ")\n",
    "\n",
    "category_total = category_trends.groupBy(\"Category\").agg(\n",
    "    _sum(\"Total_Revenue\").alias(\"Category_Total_Revenue\")\n",
    ")\n",
    "\n",
    "# Get the top 25 categories by total revenue\n",
    "\n",
    "top_25_categories = category_total.orderBy(col(\"Category_Total_Revenue\").desc()).limit(25)\n",
    "\n",
    "\n",
    "# Filter category_trends to include only top 25 categories\n",
    "\n",
    "top_25_category_list = [row[\"Category\"] for row in top_25_categories.collect()]\n",
    "category_trends_filtered = category_trends.filter(col(\"Category\").isin(top_25_category_list))\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "category_trends_pd = category_trends_filtered.toPandas()\n",
    "\n",
    "category_trends_pd[\"Month_Year\"] = category_trends_pd[\"order_year\"].astype(str) + \"-\" + \\\n",
    "                                    category_trends_pd[\"order_month\"].astype(str).str.zfill(2)\n",
    "\n",
    "\n",
    "# Plot revenue trends for top 25 categories\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.lineplot(data=category_trends_pd, x=\"Month_Year\", y=\"Total_Revenue\", hue=\"Category\", marker=\"o\")\n",
    "plt.title(\"Monthly Revenue Trends for Top 25 Product Categories\")\n",
    "plt.xlabel(\"Month-Year\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Top_25_Category_Trends.png\")\n",
    "s3.upload_file(\"Top_25_Category_Trends.png\", \"customeranalysis123\", \"Top_25_Category_Trends.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_JGx9raEl1v"
   },
   "source": [
    "### 4.2.7 Assess how bulk purchases affect revenue and supply chain operations <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Analyse the impact of how bulk purchasing behavior affects revenue and the overall supply chain operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "06SrupNpurIR"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dd1fe2c21c48feb47e99b9bede431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Assess how bulk purchases affect revenue and supply chain operations\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter bulk purchases (Quantity > 5) and compute total revenue per category\n",
    "\n",
    "bulk_purchases = merged_data.filter(col(\"Quantity\") > 5)\n",
    "\n",
    "bulk_category_revenue = bulk_purchases.groupBy(\"Category\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Bulk_Total_Revenue\")\n",
    ")\n",
    "\n",
    "# Select the top 25 categories by total revenue\n",
    "\n",
    "top_25_bulk_categories = bulk_category_revenue.orderBy(col(\"Bulk_Total_Revenue\").desc()).limit(25)\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "\n",
    "top_25_bulk_pd = top_25_bulk_categories.toPandas().sort_values(by=\"Bulk_Total_Revenue\", ascending=False)\n",
    "\n",
    "\n",
    "# Plot revenue from bulk purchases (Top 25 categories)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=top_25_bulk_pd, y=\"Category\", x=\"Bulk_Total_Revenue\", palette=\"coolwarm\")\n",
    "plt.title(\"Top 25 Categories by Revenue from Bulk Purchases (Quantity > 5)\")\n",
    "plt.xlabel(\"Bulk Purchase Revenue\")\n",
    "plt.ylabel(\"Product Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Bulk_Purchase_Top25_Categories.png\")\n",
    "s3.upload_file(\"Bulk_Purchase_Top25_Categories.png\", \"customeranalysis123\", \"Bulk_Purchase_Top25_Categories.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw0IyI5PEnf8"
   },
   "source": [
    "### 4.2.8 Compare lifecycle strategies <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Compare new and established products to inform and compare lifecycle strategies to make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "QzNAm_tiwOmi"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24af753991ec404a841ac647791526ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare new and established products to inform lifecycle strategies\n",
    "\n",
    "from pyspark.sql.functions import year, min as _min, sum as _sum, col\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "merged_data = merged_data.withColumn(\"order_year\", year(col(\"order_date_parsed\")))\n",
    "\n",
    "# Compute \"Launch Year\" as the first recorded sale year for each product\n",
    "\n",
    "product_launch_year = merged_data.groupBy(\"Title\").agg(\n",
    "    _min(\"order_year\").alias(\"Launch_Year\")\n",
    ")\n",
    "\n",
    "# Join this back to the main dataset\n",
    "\n",
    "merged_with_launch = merged_data.join(product_launch_year, on=\"Title\", how=\"left\")\n",
    "\n",
    "\n",
    "# Now, we can compute revenue by launch year\n",
    "\n",
    "revenue_by_launch_year = merged_with_launch.groupBy(\"Launch_Year\").agg(\n",
    "    _sum(\"Total_Spend\").alias(\"Total_Revenue\")\n",
    ").orderBy(\"Launch_Year\")\n",
    "\n",
    "\n",
    "# Convert to Pandas\n",
    "\n",
    "revenue_by_launch_year_pd = revenue_by_launch_year.toPandas()\n",
    "\n",
    "# Plot revenue vs. launch year\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=revenue_by_launch_year_pd, x=\"Launch_Year\", y=\"Total_Revenue\", palette=\"crest\")\n",
    "plt.title(\"Revenue by Product Launch Year\")\n",
    "plt.xlabel(\"Launch Year\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Revenue_by_Launch_Year.png\")\n",
    "s3.upload_file(\"Revenue_by_Launch_Year.png\", \"customeranalysis123\", \"Revenue_by_Launch_Year.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JYvEA5GEpLw"
   },
   "source": [
    "#5 Conclusion <font color = red>[10 marks]</font> <br>\n",
    "\n",
    "Write your conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nYhs8T4w4jI"
   },
   "source": [
    "### In this project, we conducted an in-depth analysis of customer purchase behavior using PySpark on a large-scale e-commerce dataset. The key objectives were to uncover trends in revenue, product performance, state-wise demand, and potential fraud indicators. Below are the major findings and insights:\n",
    "\n",
    "## a) Weekly Sales Trends: Schedule marketing campaigns and promotions around Sundays and Mondays to maximize customer engagement.\n",
    "## b) Top-Selling Products :Focus inventory and marketing efforts on these top performers for better ROI.\n",
    "## c) State Wise Revenue Distribution:Invest in targeted ads and logistics in high-performing states; explore growth strategies in underperforming regions.\n",
    "## d) Bulk Behaviour Purchase:ailor bulk discount campaigns for top bulk-buying categories to optimize inventory turnover.\n",
    "## e) Repeat Purchase Analysis: Develop retention initiatives like loyalty programs or email follow-ups to convert one-time buyers into repeat customers(Repeat customer data useful for analysis)\n",
    "## f) Suspicious Transaction Detection:Investigate flagged transactions further to distinguish between genuine bulk purchases and fraud attempts.\n",
    "## g) Category Wise Demand Trends:Use this data for inventory planning and seasonal promotions to match demand cycles.\n",
    "## h) Product Lifecycle Analysis: Maintain support for high-performing old products while strategically investing in launch and promotion of new items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This analysis provides valuable direction for improving:\n",
    "\n",
    "#Marketing strategies (based on time and region),\n",
    "\n",
    "#Inventory management (through category trends and bulk behavior),\n",
    "\n",
    "#Customer retention, and\n",
    "\n",
    "#Fraud prevention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sZY6vIiWEPOWoKajaiSQ5yGTHXUWhKxg",
     "timestamp": 1739959540968
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
